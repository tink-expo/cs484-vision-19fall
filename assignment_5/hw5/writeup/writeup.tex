%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CS484 Written Question Template
%
% Acknowledgements:
% The original code is written by Prof. James Tompkin (james_tompkin@brown.edu).
% The second version is revised by Prof. Min H. Kim (minhkim@kaist.ac.kr).
%
% This is a LaTeX document. LaTeX is a markup language for producing 
% documents. Your task is to fill out this document, then to compile 
% it into a PDF document. 
%
% 
% TO COMPILE:
% > pdflatex thisfile.tex
%
% If you do not have LaTeX and need a LaTeX distribution:
% - Personal laptops (all common OS): www.latex-project.org/get/
% - We recommend latex compiler miktex (https://miktex.org/) for windows,
%   macTex (http://www.tug.org/mactex/) for macOS users.
%   And TeXstudio(http://www.texstudio.org/) for latex editor.
%   You should install both compiler and editor for editing latex.
%   The another option is Overleaf (https://www.overleaf.com/) which is 
%   an online latex editor.
%
% If you need help with LaTeX, please come to office hours. 
% Or, there is plenty of help online:
% https://en.wikibooks.org/wiki/LaTeX
%
% Good luck!
% Min and the CS484 staff
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% How to include two graphics on the same line:
% 
% \includegraphics[\width=0.49\linewidth]{yourgraphic1.png}
% \includegraphics[\width=0.49\linewidth]{yourgraphic2.png}
%
% How to include equations:
%
% \begin{equation}
% y = mx+c
% \end{equation}
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue]{hyperref}
\usepackage[a4paper,margin=1.5in]{geometry}
\usepackage{stackengine,graphicx}
\usepackage{fancyhdr}
\setlength{\headheight}{15pt}
\usepackage{microtype}
\usepackage{times}
\usepackage{booktabs}

% From https://ctan.org/pkg/matlab-prettifier
\usepackage[numbered,framed]{matlab-prettifier}

\frenchspacing
\setlength{\parindent}{0cm} % Default is 15pt.
\setlength{\parskip}{0.3cm plus1mm minus1mm}

\pagestyle{fancy}
\fancyhf{}
\lhead{Homework Writeup}
\rhead{CS484}
\rfoot{\thepage}

\date{}

\title{\vspace{-1cm}Homework 5 Writeup}


\begin{document}
\maketitle
\vspace{-3cm}
\thispagestyle{fancy}

\section*{Instructions}
\begin{itemize}
  \item Describe any interesting decisions you made to write your algorithm.
  \item Show and discuss the results of your algorithm.
  \item Feel free to include code snippets, images, and equations.
  \item Use as many pages as you need, but err on the short side If you feel you only need to write a short amount to meet the brief, th
  
  \item \textbf{Please make this document anonymous.}
\end{itemize}

\lstset{style=Matlab-editor}

\textbf{[NOTE]} \\
I wrote my code under assumption that it is tested in \textbf{MATLAB 2019b}. If it is run in lower versions, it may cause errors. Please test it in 2019b.

\section*{Implementation}

\textbf{Representation - Tiny images} \\
I made tiny image representation by applying below code to each double image.
\begin{lstlisting}
% To tiny img.
img = imresize(img, [16, 16]);
img = reshape(img, [1, 16 * 16]);

% zero mean and unit length.
img = img - mean(img);
img = img / norm(img);
\end{lstlisting}

\textbf{Representation - Bag of words} \\
First, I extracted features from images and clustered them to build a vocabulary.
For extracting features, I used MATLAB \lstinline{extractHOGFeatures} function.
Instead of extracting at every 20x20 grid points, I used interest points detected by MATLAB \lstinline{detectSURFFeatures}.
\begin{lstlisting}
interest_points = detectSURFFeatures(img, 'MetricThreshold', 100);
interest_points = selectStrongest(interest_points, 200);
[features, ~] = extractHOGFeatures(img, interest_points, 'CellSize', [16 16]);
features_all = cat(1, features_all, features);
\end{lstlisting}
After concatenating the features extracted from each images by above code,
I clustered them as below.
\begin{lstlisting}
[~, vocab] = kmeans(features_all, vocab_size);
\end{lstlisting}
Using the vocabulary built by above procedure, each images can be represented by a bag of words.
I extracted features for each images using the same method described above. 
This could be seen as a redundant procedure, but since we save the vocabulary matrix and do not perform building vocabulary each time,
so actually it is not.
Then, I built a histogram for each images - which is a bag of word reperesentation - using the code below.
The histogram represents the frequency of words in vocabulary that was defined that is closest to the each features extracted from the image.
In order to make this representation reasonable even if the number of detected features differs by images, I applied unit size normalization to the histogram.
\begin{lstlisting}
histogram = zeros(1, vocab_size);
for j = 1 : size(features, 1)
    dist = vocab - features(j, :);
    dist = (vecnorm(dist'))';
    [~, vocab_ind] = min(dist);
    histogram(vocab_ind) = histogram(vocab_ind) + 1;
end
histogram = histogram / norm(histogram);
image_feats(i, :) = histogram;
\end{lstlisting}

\textbf{Classification - Nearest neighbor} \\
I used voting based on k nearest neighbors, with k set to 7.
For each test image features, I selected k nearest neighbor by below code.
\begin{lstlisting}
diffs = train_image_feats - test_image_feats(i, :);
diffs = vecnorm(diffs');
[~, inds] = mink(diffs, k, 2);
\end{lstlisting}
Then I got the prediction result by voting using function below.
\begin{lstlisting}
function max_label = get_voted_label(inds, train_labels)

k = size(inds, 2);
found_labels = cell(k, 1);
for i = 1 : k
    found_labels{i} = train_labels{inds(i)};
end
votes = zeros(k, 1);
for i = 1 : k
    votes(i) = nnz(strcmp(found_labels{i}, found_labels));
end
[~, vote_ind] = max(votes);
max_label = found_labels{vote_ind};

end
\end{lstlisting}

\textbf{Classification - Support vector machine} \\
In order to make a multi-class SVM, I first trained binary (true or false) SVM for each categories by using the code below.
I set the \lstinline{Lambda} parameter to \lstinline{4.8e-04} by manual testing.
\begin{lstlisting}
for i = 1 : num_categories
    binary_vector = strcmp(train_labels, categories(i));
    svm_models{i} = fitclinear(train_image_feats, binary_vector, 'ClassNames', [false true], 'Lambda', 4.8e-04);
end
\end{lstlisting}
Then, each images are predicted by the binary SVMs of every categories. 
I set the prediction result to be the category that gives the highest score at binary prediction for "true" label.
\begin{lstlisting}
m = size(test_image_feats, 1);
scores_all = zeros(m, num_categories);
for i = 1 : num_categories
    [~, scores_category] = predict(svm_models{i}, test_image_feats);
    scores_all(:, i) = scores_category(:, 2);
end
[~, categories_inds] = max(scores_all, [], 2);
predicted_categories = categories(categories_inds);
\end{lstlisting}

\section*{Results}

Elapsed time of \lstinline{build_vocabulary.m} was 116.083s. \\
Below describes the elapsed times excluding \lstinline{build_vocabulary.m} and accuracies of each methods.
\begin{table}[h]
    \centering
    \begin{tabular}{ccc}
        \toprule
        Methods & Accuracy (\%) & Elapsed time (s) \\
        \midrule
        Tiny images \& Nearest neighbor & 23.2 & 7.887 \\
        Bag of words \& Nearest neighbor & 44.9 & 80.710 \\
        Bag of words \& Support vector machine & 55.0 & 79.898 \\
        \bottomrule
    \end{tabular}
    \caption{Accuracy and elapsed times comparison of each methods.}
    \label{tab:table1}
\end{table}

\pagebreak
\begin{figure}[h]
    \centering
    \includegraphics[width=5cm]{tiny_nearest.png}
    \caption{Confusion matrix result from \textit{Tiny images \& Nearest neighbor}.}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=5cm]{bow_nearest.png}
    \caption{Confusion matrix result from \textit{Bag of words \& Nearest neighbor}.}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=5cm]{bow_svm.png}
    \caption{Confusion matrix result from \textit{Bag of words \& Support vector machine}.}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=6cm]{table.png}
    \caption{Classification result table from \textit{Bag of words \& Support vector machine}. This method setup had the best accuracy, and its elapsed time was slightly shorter than the second best accuracy method.}
\end{figure}

\end{document}
